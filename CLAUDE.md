# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

End-to-end music generation pipeline: **Stereo Audio → Stems/MIDI → Event Preprocessing → Transformer Training → MIDI Generation**

The audio→MIDI stage lives in a git submodule at `vendor/all-in-one-ai-midi-pipeline/`.

## Common Commands

### Environment Setup
```bash
bash setup.bash                    # uv-based venv in .venv-ai-music (Python 3.10)
source .venv-ai-music/bin/activate
```

### Full Pipeline
```bash
scripts/run_end_to_end.sh                              # all instruments
scripts/run_end_to_end.sh --tracks drums,bass,guitar   # subset
scripts/run_end_to_end.sh --device mps                 # force device
```

### Individual Stages
```bash
# 1. Audio→MIDI (run from vendor/all-in-one-ai-midi-pipeline/)
python pipeline.py run-batch "data/raw/*.wav"
python pipeline.py export-midi --out out_midis

# 2. Preprocess MIDI→events
python training/pre.py --midi_folder out_midis --data_folder runs/events
python training/pre.py --midi_folder out_midis --data_folder runs/events --tracks drums,bass

# 3. Train
python training/train.py \
  --data_dir runs/events \
  --train_pkl runs/events/events_train.pkl \
  --val_pkl runs/events/events_val.pkl \
  --vocab_json runs/events/event_vocab.json \
  --save_path runs/checkpoints/es_model.pt \
  --device auto

# 4. Generate
python training/generate.py \
  --ckpt runs/checkpoints/es_model.pt \
  --vocab_json runs/events/event_vocab.json \
  --out_midi runs/generated/out.mid \
  --device auto
```

### Tests
No test framework is configured yet. Tests should go in `./tests/` using pytest.

## Architecture

### Pipeline Stages

1. **Audio→MIDI** (`vendor/all-in-one-ai-midi-pipeline/`): HTDemucs stem separation → Basic Pitch (melodic) + ADTOF (drums) transcription → multi-track MIDI with 6 canonical instruments.

2. **Preprocessing** (`training/pre.py`): Multi-track MIDI → event token sequences. 512-token windows with 256-stride. Train-only augmentation (±1 semitone pitch, ±10 velocity). Produces `events_train.pkl`, `events_val.pkl`, `event_vocab.json`. Also computes a 34-dim polyphony instructor (auxiliary training target).

3. **Training** (`training/train.py`): Transformer encoder with factored output — type classification head + per-type value heads. Auto-scales model capacity based on dataset size. Joint loss: token prediction (type 20% + value 80%) + auxiliary polyphony prediction (20%). Default: D_MODEL=192, N_HEADS=6, N_LAYERS=4.

4. **Generation** (`training/generate.py`): Autoregressive generation with grammar-constrained state machine (TIME→INST→VEL→PITCH→DUR). Temperature + nucleus sampling. Adaptive rhythmic grid snapping (straight vs triplet). Outputs MIDI + event JSON + metadata.

### Canonical Instruments
Six fixed instrument labels: `voxlead`, `voxharm`, `guitar`, `other`, `bass`, `drums`. Event order per note: `TIME_SHIFT → BAR → INST → VEL → PITCH → DUR`.

### Key Data Formats
- **Event vocab** (`event_vocab.json`): ~2400 tokens including specialized pitch spaces (melodic vs percussion)
- **Training data** (`events_*.pkl`): Pickled event sequences (only load from trusted sources)
- **Checkpoints** (`.pt`): `torch.save()` dict with model state + vocab + config metadata

### Output Directories (all git-ignored)
- `out_midis/` — exported MIDIs from audio→MIDI stage
- `runs/events/` — preprocessed event datasets
- `runs/checkpoints/` — trained model checkpoints
- `runs/generated/` — generated MIDI outputs
- `data/raw/` — input audio files

## Security Notes

- **Pickle loading** in `train.py` uses bare `pickle.load()` — only load training data generated by this pipeline.
- **torch.load** in `generate.py` does not use `weights_only=True` — only load checkpoints generated by this pipeline.
